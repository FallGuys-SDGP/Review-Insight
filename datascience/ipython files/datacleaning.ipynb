{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dJ-JteJxzFb4",
    "outputId": "a5fe073d-eb55-48c1-c4fb-ffcff21ac8d4"
   },
   "outputs": [],
   "source": [
    "!pip install contractions \n",
    "import contractions\n",
    "from contractions import CONTRACTION_MAP\n",
    "# from contractions import contractions_dict\n",
    "\n",
    "# !{sys.executable} -m pip install contractions\n",
    "\n",
    "# import sys, setuptools, tokenize\n",
    "# !pip install pycontractions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "daZaPMsizFb8"
   },
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> imports >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re, unicodedata, string  \n",
    "# import num2word\n",
    "!pip install inflect\n",
    "import inflect\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import sys, setuptools, tokenize\n",
    "\n",
    "# import sys\n",
    "\n",
    "# import pycontractions \n",
    "# from contractions import CONTRACTION_MAP\n",
    "# from contractions import CONTRACTION_dict\n",
    "# from pycontractions import Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "id": "GTT8DtTVzFb-",
    "outputId": "d9339de4-00f6-4a58-89cb-61bbf1b87b22"
   },
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> loading the dataset >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "# filename = '../dataset/Reviews only (Sentiment).csv'\n",
    "# file = open(filename, 'rt')\n",
    "# text = file.read()\n",
    "# file.close()\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "df1 = pd.read_csv('../dataset/testing.csv')\n",
    "# df1.set_index('reviewText', inplace=True)\n",
    "df = df1\n",
    "# df_no_indices = df.to_string(index=False)\n",
    "# print(df_no_indices)\n",
    "# df = df.style.hide_index()\n",
    "df\n",
    "# df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R52Vf3VJzFb-",
    "outputId": "15c21ae1-4f44-4ab0-f6fa-3d417fd4ccb3"
   },
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> removing null values >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "# counting rows before removing null values\n",
    "df.shape\n",
    "df['reviewText']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tptyO5PUzFb_"
   },
   "outputs": [],
   "source": [
    "#removing null values\n",
    "# counting rows after removing null values\n",
    "df.dropna(subset=['reviewText'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "id": "HUfVAjsHzFcA",
    "outputId": "e61cb83e-91a0-4818-ff48-5359d58393a4"
   },
   "outputs": [],
   "source": [
    "# after removing null values\n",
    "# before removing URL\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "id": "zjapcP_8zFcA",
    "outputId": "f01368e8-fdcb-422e-f188-fbf1551d8aef"
   },
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> removing URL >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "def remove_URL(reviewText):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'', str(reviewText))\n",
    "\n",
    "df['reviewText']=df['reviewText'].apply(remove_URL)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "id": "AvFQGrLYzFcB",
    "outputId": "177bae9c-f804-4fa3-e29f-8780df298c38"
   },
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> removing HTML tags >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "def remove_html(reviewText):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',str(reviewText))\n",
    "df['reviewText'] = df['reviewText'].apply(remove_html)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "id": "z5RKdymYzFcB",
    "outputId": "83192eff-d9af-4d4e-f482-78ffbcce473c"
   },
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> removing square brackets and the inside of the square bracket >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "def remove_between_square_brackets(reviewText):\n",
    "    return re.sub('\\[[^]]*\\]', '', str(reviewText))\n",
    "df['reviewText'] = df['reviewText'].apply(remove_between_square_brackets)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "id": "fs7XoM77zFcC",
    "outputId": "a4223198-cffe-4867-a0c2-f0d435cfd199"
   },
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> removing Pictures/Tags/Symbols/Emojis >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "def remove_emoji(reviewText):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', str(reviewText))\n",
    "\n",
    "df['reviewText'] = df['reviewText'].apply(remove_emoji)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-aRr_2IDzFcC"
   },
   "outputs": [],
   "source": [
    "# import re\n",
    "# cList = {\n",
    "#   \"ain't\": \"am not\",\n",
    "#   \"aren't\": \"are not\",\n",
    "#   \"can't\": \"cannot\",\n",
    "#   \"can't've\": \"cannot have\",\n",
    "#   \"'cause\": \"because\",\n",
    "#   \"could've\": \"could have\",\n",
    "#   \"couldn't\": \"could not\",\n",
    "#   \"couldn't've\": \"could not have\",\n",
    "#   \"didn't\": \"did not\",\n",
    "#   \"doesn't\": \"does not\",\n",
    "#   \"don't\": \"do not\",\n",
    "#   \"hadn't\": \"had not\",\n",
    "#   \"hadn't've\": \"had not have\",\n",
    "#   \"hasn't\": \"has not\",\n",
    "#   \"haven't\": \"have not\",\n",
    "#   \"he'd\": \"he would\",\n",
    "#   \"he'd've\": \"he would have\",\n",
    "#   \"he'll\": \"he will\",\n",
    "#   \"he'll've\": \"he will have\",\n",
    "#   \"he's\": \"he is\",\n",
    "#   \"how'd\": \"how did\",\n",
    "#   \"how'd'y\": \"how do you\",\n",
    "#   \"how'll\": \"how will\",\n",
    "#   \"how's\": \"how is\",\n",
    "#   \"I'd\": \"I would\",\n",
    "#   \"I'd've\": \"I would have\",\n",
    "#   \"I'll\": \"I will\",\n",
    "#   \"I'll've\": \"I will have\",\n",
    "#   \"I'm\": \"I am\",\n",
    "#   \"I've\": \"I have\",\n",
    "#   \"isn't\": \"is not\",\n",
    "#   \"it'd\": \"it had\",\n",
    "#   \"it'd've\": \"it would have\",\n",
    "#   \"it'll\": \"it will\",\n",
    "#   \"it'll've\": \"it will have\",\n",
    "#   \"it's\": \"it is\",\n",
    "#   \"let's\": \"let us\",\n",
    "#   \"ma'am\": \"madam\",\n",
    "#   \"mayn't\": \"may not\",\n",
    "#   \"might've\": \"might have\",\n",
    "#   \"mightn't\": \"might not\",\n",
    "#   \"mightn't've\": \"might not have\",\n",
    "#   \"must've\": \"must have\",\n",
    "#   \"mustn't\": \"must not\",\n",
    "#   \"mustn't've\": \"must not have\",\n",
    "#   \"needn't\": \"need not\",\n",
    "#   \"needn't've\": \"need not have\",\n",
    "#   \"o'clock\": \"of the clock\",\n",
    "#   \"oughtn't\": \"ought not\",\n",
    "#   \"oughtn't've\": \"ought not have\",\n",
    "#   \"shan't\": \"shall not\",\n",
    "#   \"sha'n't\": \"shall not\",\n",
    "#   \"shan't've\": \"shall not have\",\n",
    "#   \"she'd\": \"she would\",\n",
    "#   \"she'd've\": \"she would have\",\n",
    "#   \"she'll\": \"she will\",\n",
    "#   \"she'll've\": \"she will have\",\n",
    "#   \"she's\": \"she is\",\n",
    "#   \"should've\": \"should have\",\n",
    "#   \"shouldn't\": \"should not\",\n",
    "#   \"shouldn't've\": \"should not have\",\n",
    "#   \"so've\": \"so have\",\n",
    "#   \"so's\": \"so is\",\n",
    "#   \"that'd\": \"that would\",\n",
    "#   \"that'd've\": \"that would have\",\n",
    "#   \"that's\": \"that is\",\n",
    "#   \"there'd\": \"there had\",\n",
    "#   \"there'd've\": \"there would have\",\n",
    "#   \"there's\": \"there is\",\n",
    "#   \"they'd\": \"they would\",\n",
    "#   \"they'd've\": \"they would have\",\n",
    "#   \"they'll\": \"they will\",\n",
    "#   \"they'll've\": \"they will have\",\n",
    "#   \"they're\": \"they are\",\n",
    "#   \"they've\": \"they have\",\n",
    "#   \"to've\": \"to have\",\n",
    "#   \"wasn't\": \"was not\",\n",
    "#   \"we'd\": \"we had\",\n",
    "#   \"we'd've\": \"we would have\",\n",
    "#   \"we'll\": \"we will\",\n",
    "#   \"we'll've\": \"we will have\",\n",
    "#   \"we're\": \"we are\",\n",
    "#   \"we've\": \"we have\",\n",
    "#   \"weren't\": \"were not\",\n",
    "#   \"what'll\": \"what will\",\n",
    "#   \"what'll've\": \"what will have\",\n",
    "#   \"what're\": \"what are\",\n",
    "#   \"what's\": \"what is\",\n",
    "#   \"what've\": \"what have\",\n",
    "#   \"when's\": \"when is\",\n",
    "#   \"when've\": \"when have\",\n",
    "#   \"where'd\": \"where did\",\n",
    "#   \"where's\": \"where is\",\n",
    "#   \"where've\": \"where have\",\n",
    "#   \"who'll\": \"who will\",\n",
    "#   \"who'll've\": \"who will have\",\n",
    "#   \"who's\": \"who is\",\n",
    "#   \"who've\": \"who have\",\n",
    "#   \"why's\": \"why is\",\n",
    "#   \"why've\": \"why have\",\n",
    "#   \"will've\": \"will have\",\n",
    "#   \"won't\": \"will not\",\n",
    "#   \"won't've\": \"will not have\",\n",
    "#   \"would've\": \"would have\",\n",
    "#   \"wouldn't\": \"would not\",\n",
    "#   \"wouldn't've\": \"would not have\",\n",
    "#   \"y'all\": \"you all\",\n",
    "#   \"y'alls\": \"you alls\",\n",
    "#   \"y'all'd\": \"you all would\",\n",
    "#   \"y'all'd've\": \"you all would have\",\n",
    "#   \"y'all're\": \"you all are\",\n",
    "#   \"y'all've\": \"you all have\",\n",
    "#   \"you'd\": \"you had\",\n",
    "#   \"you'd've\": \"you would have\",\n",
    "#   \"you'll\": \"you you will\",\n",
    "#   \"you'll've\": \"you you will have\",\n",
    "#   \"you're\": \"you are\",\n",
    "#   \"you've\": \"you have\"\n",
    "# }\n",
    "\n",
    "# c_re = re.compile('(%s)' % '|'.join(cList.keys()))\n",
    "\n",
    "# def expandContractions(reviewText, c_re=c_re):\n",
    "#     def replace(match):\n",
    "#         return cList[match.group(0)]\n",
    "#     return c_re.sub(replace, reviewText)\n",
    "\n",
    "# # examples\n",
    "# # print expandContractions('Don\\'t you get it?')\n",
    "# # print expandContractions('I ain\\'t got time for y\\'alls foolishness')\n",
    "# # print expandContractions('You won\\'t live to see tomorrow.')\n",
    "# # print expandContractions('You\\'ve got serious cojones coming in here like that.')\n",
    "# # print expandContractions('I hadn\\'t\\'ve enough')\n",
    "\n",
    "# df['reviewText'] = df['reviewText'].apply(expandContractions)\n",
    "# df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387
    },
    "id": "jcLF1Z4czFcE",
    "outputId": "7c111af3-f084-4a2a-eb19-220099fac282"
   },
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> expand contractions >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "def expand_contractions(reviewText):\n",
    "    contractionsPattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())),flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        firstChar = match[0]\n",
    "        expandedContraction = CONTRACTION_MAP.get(match) \\\n",
    "            if CONTRACTION_MAP.get(match) \\\n",
    "            else CONTRACTION_MAP.get(match.lower())\n",
    "        expandedContraction = firstChar+expandedContraction[1:]\n",
    "        return expandedContraction\n",
    "    reviewText = contractionsPattern.sub(expand_match, str(reviewText))\n",
    "    reviewText = re.sub(\"'\", \"\", reviewText)\n",
    "    return reviewText\n",
    "\n",
    "df['reviewText']=df['reviewText'].apply(expand_contractions)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hzLJFvjjzFcF"
   },
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> tokenizing words >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "# ndf = df['reviewText']\n",
    "\n",
    "# tokens = word_tokenize(str(df))\n",
    "# print(tokens[:200])\n",
    "\n",
    "tokens = [w for t in (df[\"reviewText\"].apply(word_tokenize)) for w in t]\n",
    "# for i in tokens:\n",
    "#     print(i)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "biidA_3AzFcF"
   },
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> removing non ascii >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "removedNonAscii = [w.encode(\"ascii\", \"ignore\").decode() for w in tokens]\n",
    "print(removedNonAscii[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vn9KtDp4zFcG"
   },
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> convert to lower case >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "lowerCase = [w.lower() for w in removedNonAscii]\n",
    "print(lowerCase[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ETwpUIh6zFcG"
   },
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> removing punctuation >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "removedPunctuation = [w.translate(table) for w in lowerCase]\n",
    "print(removedPunctuation[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0O32QegizFcG"
   },
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> replacing numbers with string  >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "p = inflect.engine()\n",
    "stringNumbers = []\n",
    "for word in removedPunctuation:\n",
    "    if word.isdigit():\n",
    "#         new_word = num2word.to_card(15)\n",
    "        new_word = p.number_to_words(word)\n",
    "        stringNumbers.append(new_word)\n",
    "    else:\n",
    "        stringNumbers.append(word)\n",
    "        \n",
    "# stringNumbers = [w.lower() for w in removedPunctuation]\n",
    "print(stringNumbers[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AXHbGL8vzFcH"
   },
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> removing stopwords >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "removedStopwords = [w for w in removedPunctuation if not w in stop_words]\n",
    "print(removedStopwords[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FtHYholRzFcH"
   },
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> stemming words >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "stems = []\n",
    "for word in removedStopwords:\n",
    "    stem = stemmer.stem(word)\n",
    "    stems.append(stem)\n",
    "    \n",
    "print(stems[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RMwzzNzWzFcI"
   },
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> lemmatize_verbs >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = []\n",
    "for word in stems:\n",
    "    lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "    lemmas.append(lemma)\n",
    "    \n",
    "print(lemmas[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "datacleaning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
