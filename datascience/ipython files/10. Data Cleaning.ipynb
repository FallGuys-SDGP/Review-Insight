{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10. Data Cleaning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwww74TVJeex"
      },
      "source": [
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> imports >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
        "!pip install inflect\n",
        "!pip install contractions \n",
        "!pip install krovetzstemmer\n",
        "import contractions\n",
        "from contractionsOne import CONTRACTION_MAP\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "from krovetzstemmer import Stemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "import inflect\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys, setuptools, tokenize\n",
        "import re, unicodedata, string  \n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSKfd340Wq7T"
      },
      "source": [
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> loading the dataset >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
        "\n",
        "pd.options.mode.chained_assignment = None\n",
        "df = pd.read_csv('/content/testing.csv')\n",
        "review_df = df\n",
        "review_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUXoev63Wv2I"
      },
      "source": [
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> removing null values >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
        "\n",
        "#removing null values\n",
        "# counting rows after removing null values\n",
        "# df.dropna(subset=['reviewText'], inplace=True)\n",
        "# df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByveAYlnWzMz"
      },
      "source": [
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> removing URL >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
        "\n",
        "def remove_URL(reviewText):\n",
        "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url.sub(r'', str(reviewText))\n",
        "df['reviewText']=df['reviewText'].apply(remove_URL)\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbd-5DaCW1eE"
      },
      "source": [
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> removing HTML tags >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
        "\n",
        "def remove_html(reviewText):\n",
        "    html=re.compile(r'<.*?>')\n",
        "    return html.sub(r'',str(reviewText))\n",
        "df['reviewText'] = df['reviewText'].apply(remove_html)\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DjhYX7kXAc9"
      },
      "source": [
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> removing square brackets and the inside of the square bracket >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
        "\n",
        "def remove_between_square_brackets(reviewText):\n",
        "    return re.sub('\\[[^]]*\\]', '', str(reviewText))\n",
        "df['reviewText'] = df['reviewText'].apply(remove_between_square_brackets)\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDi5drdwXBpd"
      },
      "source": [
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> removing Pictures/Tags/Symbols/Emojis >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
        "\n",
        "def remove_emoji(reviewText):\n",
        "    emoj = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u2640-\\u2642\" \n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\ufe0f\"  # dingbats\n",
        "        u\"\\u3030\"\n",
        "                      \"]+\", re.UNICODE)\n",
        "    return re.sub(emoj, '', str(reviewText))\n",
        "\n",
        "df['reviewText'] = df['reviewText'].apply(remove_emoji)\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zN5lEJEXHvx"
      },
      "source": [
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> removing non ascii >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
        "\n",
        "df['reviewText'] = [w.encode(\"ascii\", \"ignore\").decode() for w in df['reviewText']]\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izbISpB-XNLO"
      },
      "source": [
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> convert to lower case >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
        "\n",
        "df['reviewText'] = [w.lower() for w in df['reviewText']]\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjNyfjm1XCKj"
      },
      "source": [
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> expand contractions >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
        "\n",
        "def expand_contractions(reviewText):\n",
        "    contractionsPattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())),flags=re.IGNORECASE|re.DOTALL)\n",
        "    def expand_match(contraction):\n",
        "        match = contraction.group(0)\n",
        "        firstChar = match[0]\n",
        "        expandedContraction = CONTRACTION_MAP.get(match) \\\n",
        "            if CONTRACTION_MAP.get(match) \\\n",
        "            else CONTRACTION_MAP.get(match.lower())\n",
        "        expandedContraction = firstChar+expandedContraction[1:]\n",
        "        return expandedContraction\n",
        "    reviewText = contractionsPattern.sub(expand_match, str(reviewText))\n",
        "    reviewText = re.sub(\"'\", \"\", reviewText)\n",
        "    return reviewText\n",
        "\n",
        "df['reviewText']=df['reviewText'].apply(expand_contractions)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIdek2gx8lB6"
      },
      "source": [
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> removing all punctuations >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
        "\n",
        "import string\n",
        "def clear_punctuation(s):\n",
        "  clear_string = \"\"\n",
        "  for symbol in s:\n",
        "    if symbol not in string.punctuation:\n",
        "      clear_string += symbol\n",
        "  return clear_string\n",
        "df['reviewText'] = df['reviewText'].apply(clear_punctuation)\n",
        "df.head(10)\n",
        "# print(clear_punctuation(df))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LAS3klxIGzq"
      },
      "source": [
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> tokenizing sentence >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
        "\n",
        "sentence_all = []\n",
        "for index, row in df.iterrows():\n",
        "  sentences_with_feature = []\n",
        "  for sen in sent_tokenize(row['reviewText']):\n",
        "    sentences_with_feature.append(sen)\n",
        "  sentence_all.append(sentences_with_feature)\n",
        "\n",
        "list_of_tuples = list(zip(sentence_all)) \n",
        "df_sentences = pd.DataFrame(list_of_tuples,columns = ['reviewText'])\n",
        "df_sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4wlI4v57Kdk"
      },
      "source": [
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> tokenizing words >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
        "\n",
        "def wordTokenize(sentence):\n",
        "    tokens = [w for t in (sentence.apply(word_tokenize)) for w in t]\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KA-6hB7HXQCq"
      },
      "source": [
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> replacing numbers with string  >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
        "\n",
        "def stringNumbers(sentence):\n",
        "  p = inflect.engine()\n",
        "  stringNumbers = []\n",
        "  for word in sentence:\n",
        "    if word.isdigit():\n",
        "      # new_word = num2word.to_card(15)\n",
        "      new_word = p.number_to_words(word)\n",
        "      stringNumbers.append(new_word)\n",
        "      \n",
        "    else:\n",
        "      stringNumbers.append(word)\n",
        "  # stringNumbers = [w.lower() for w in removedPunctuation]\n",
        "  return stringNumbers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaNEo8AjXQIK"
      },
      "source": [
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> removing stopwords >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "def stopWords(sentence):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  removedStopwords = [word for word in sentence if not word in stop_words]\n",
        "  return removedStopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J5WYEGRXQMu"
      },
      "source": [
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> stemming words >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
        "\n",
        "def stemmers(sentence):\n",
        "  #stemmer = PorterStemmer()\n",
        "  # stemmer = LancasterStemmer()\n",
        "  stemmer = Stemmer()\n",
        "  stems = []\n",
        "  for word in sentence:\n",
        "      stem = stemmer.stem(word)\n",
        "      stems.append(stem)\n",
        "  return stems"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGyCK3T7XQR4"
      },
      "source": [
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> lemmatize_verbs >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
        "\n",
        "def lemmatizers(sentence):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  lemmas = []\n",
        "  for word in sentence:\n",
        "      lemma = lemmatizer.lemmatize(word, pos='v')\n",
        "      lemmas.append(lemma)\n",
        "  return lemmas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPIHEkT8MrKk"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "alltokens = []\n",
        "wordTokens = pd.DataFrame()\n",
        "for index, row in df_sentences.iterrows():\n",
        "  list_of_tuples = list(zip(row['reviewText'])) \n",
        "  df_sentences_w = pd.DataFrame(list_of_tuples,columns = ['reviewText'])\n",
        "  \n",
        "  wordTokens = wordTokenize(df_sentences_w['reviewText'])\n",
        "  stringNumber = stringNumbers(wordTokens)\n",
        "  stopword = stopWords(stringNumber)\n",
        "  stemmer = stemmers(stopword)\n",
        "  lemmatizer = lemmatizers(stemmer)\n",
        "  alltokens.append(lemmatizer)\n",
        "alltokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wohvx31TS0pD"
      },
      "source": [
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> de-tokenise >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
        "\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "detokenizeall = []\n",
        "for alltokensRow in alltokens:\n",
        "  reviewWordDetokenize = TreebankWordDetokenizer().detokenize(alltokensRow)\n",
        "  detokenizeall.append(reviewWordDetokenize)\n",
        "\n",
        "review_df['cleaned'] = detokenizeall\n",
        "review_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_BAn_QLfuMt"
      },
      "source": [
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> saving the cleaned data >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
        "\n",
        "review_df.to_csv('testing_cleaned_with_lemmatized.csv') \n",
        "review_df.to_csv('testing_cleaned_without_lemmatized.csv') \n",
        "\n",
        "review_df"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}